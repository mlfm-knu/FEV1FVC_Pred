{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac66f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Metrics for fold_5_predictions.csv ---\n",
      "Mean Absolute Error (MAE): 0.0397\n",
      "Mean Squared Error (MSE): 0.0026\n",
      "Root Mean Squared Error (RMSE): 0.0512\n",
      "R-squared (R2): 0.8074\n",
      "\n",
      "Evaluation complete. Metrics saved to 'rs/tabpfn_predictions_8532\\evaluated_metrics_fold_5.txt'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- Configuration ---\n",
    "PREDICTIONS_DIR = 'rs/tabpfn_predictions_8532'\n",
    "FOLD = 5\n",
    "PREDICTIONS_FILE = f'fold_{FOLD}_predictions.csv'\n",
    "METRICS_OUTPUT_FILE = f'evaluated_metrics_fold_{FOLD}.txt'\n",
    "\n",
    "# --- Main Evaluation Script ---\n",
    "def evaluate_predictions():\n",
    "    \"\"\"\n",
    "    Reads a predictions CSV file, calculates regression metrics,\n",
    "    and saves them to a text file.\n",
    "    \"\"\"\n",
    "    predictions_path = os.path.join(PREDICTIONS_DIR, PREDICTIONS_FILE)\n",
    "    output_path = os.path.join(PREDICTIONS_DIR, METRICS_OUTPUT_FILE)\n",
    "\n",
    "    if not os.path.exists(predictions_path):\n",
    "        print(f\"Error: Predictions file not found at '{predictions_path}'.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        # Load the predictions file\n",
    "        predictions_df = pd.read_csv(predictions_path)\n",
    "\n",
    "        # Check for required columns\n",
    "        if 'Ground_Truth' not in predictions_df.columns or 'Prediction' not in predictions_df.columns:\n",
    "            print(\"Error: The predictions CSV must contain 'Ground_Truth' and 'Prediction' columns.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        y_true = predictions_df['Ground_Truth']\n",
    "        y_pred = predictions_df['Prediction']\n",
    "\n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "        # Print metrics to console\n",
    "        print(f\"--- Evaluation Metrics for {PREDICTIONS_FILE} ---\")\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "        print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "        print(f\"R-squared (R2): {r2:.4f}\")\n",
    "\n",
    "        # Save metrics to a text file\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(f\"--- Evaluation Metrics for {PREDICTIONS_FILE} ---\\n\")\n",
    "            f.write(f\"Mean Absolute Error (MAE): {mae:.4f}\\n\")\n",
    "            f.write(f\"Mean Squared Error (MSE): {mse:.4f}\\n\")\n",
    "            f.write(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\\n\")\n",
    "            f.write(f\"R-squared (R2): {r2:.4f}\\n\")\n",
    "        \n",
    "        print(f\"\\nEvaluation complete. Metrics saved to '{output_path}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during evaluation: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_predictions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
