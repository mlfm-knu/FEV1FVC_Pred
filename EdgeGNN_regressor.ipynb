{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aa49abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading airway trees and target...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cnkhanh\\.conda\\envs\\ml_env\\lib\\site-packages\\torch_geometric\\deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "2025-08-25 14:06:28,916 - INFO - Starting GNN training...\n",
      "c:\\Users\\cnkhanh\\.conda\\envs\\ml_env\\lib\\site-packages\\torch_geometric\\warnings.py:17: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "C:\\Users\\cnkhanh\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\cnkhanh\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([31])) that is different to the input size (torch.Size([31, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-08-25 14:06:29,274 - INFO - Epoch 1/500, Loss: 0.3590\n",
      "2025-08-25 14:06:29,364 - INFO - Epoch 2/500, Loss: 0.0189\n",
      "2025-08-25 14:06:29,450 - INFO - Epoch 3/500, Loss: 0.0154\n",
      "2025-08-25 14:06:29,535 - INFO - Epoch 4/500, Loss: 0.0152\n",
      "2025-08-25 14:06:29,623 - INFO - Epoch 5/500, Loss: 0.0153\n",
      "2025-08-25 14:06:29,709 - INFO - Epoch 6/500, Loss: 0.0150\n",
      "2025-08-25 14:06:29,794 - INFO - Epoch 7/500, Loss: 0.0150\n",
      "2025-08-25 14:06:29,883 - INFO - Epoch 8/500, Loss: 0.0151\n",
      "2025-08-25 14:06:29,969 - INFO - Epoch 9/500, Loss: 0.0150\n",
      "2025-08-25 14:06:30,055 - INFO - Epoch 10/500, Loss: 0.0150\n",
      "2025-08-25 14:06:30,141 - INFO - Epoch 11/500, Loss: 0.0151\n",
      "2025-08-25 14:06:30,225 - INFO - Epoch 12/500, Loss: 0.0149\n",
      "2025-08-25 14:06:30,310 - INFO - Epoch 13/500, Loss: 0.0150\n",
      "2025-08-25 14:06:30,396 - INFO - Epoch 14/500, Loss: 0.0152\n",
      "2025-08-25 14:06:30,481 - INFO - Epoch 15/500, Loss: 0.0150\n",
      "2025-08-25 14:06:30,569 - INFO - Epoch 16/500, Loss: 0.0152\n",
      "2025-08-25 14:06:30,654 - INFO - Epoch 17/500, Loss: 0.0149\n",
      "2025-08-25 14:06:30,739 - INFO - Epoch 18/500, Loss: 0.0149\n",
      "2025-08-25 14:06:30,824 - INFO - Epoch 19/500, Loss: 0.0150\n",
      "2025-08-25 14:06:30,909 - INFO - Epoch 20/500, Loss: 0.0151\n",
      "2025-08-25 14:06:30,994 - INFO - Epoch 21/500, Loss: 0.0148\n",
      "2025-08-25 14:06:31,080 - INFO - Epoch 22/500, Loss: 0.0149\n",
      "2025-08-25 14:06:31,165 - INFO - Epoch 23/500, Loss: 0.0150\n",
      "2025-08-25 14:06:31,250 - INFO - Epoch 24/500, Loss: 0.0149\n",
      "2025-08-25 14:06:31,336 - INFO - Epoch 25/500, Loss: 0.0148\n",
      "2025-08-25 14:06:31,421 - INFO - Epoch 26/500, Loss: 0.0149\n",
      "2025-08-25 14:06:31,515 - INFO - Epoch 27/500, Loss: 0.0151\n",
      "2025-08-25 14:06:31,604 - INFO - Epoch 28/500, Loss: 0.0150\n",
      "2025-08-25 14:06:31,690 - INFO - Epoch 29/500, Loss: 0.0150\n",
      "2025-08-25 14:06:31,779 - INFO - Epoch 30/500, Loss: 0.0149\n",
      "2025-08-25 14:06:31,900 - INFO - Epoch 31/500, Loss: 0.0149\n",
      "2025-08-25 14:06:31,987 - INFO - Epoch 32/500, Loss: 0.0148\n",
      "2025-08-25 14:06:32,076 - INFO - Epoch 33/500, Loss: 0.0150\n",
      "2025-08-25 14:06:32,173 - INFO - Epoch 34/500, Loss: 0.0148\n",
      "2025-08-25 14:06:32,261 - INFO - Epoch 35/500, Loss: 0.0148\n",
      "2025-08-25 14:06:32,349 - INFO - Epoch 36/500, Loss: 0.0149\n",
      "2025-08-25 14:06:32,443 - INFO - Epoch 37/500, Loss: 0.0148\n",
      "2025-08-25 14:06:32,531 - INFO - Epoch 38/500, Loss: 0.0151\n",
      "2025-08-25 14:06:32,629 - INFO - Epoch 39/500, Loss: 0.0150\n",
      "2025-08-25 14:06:32,717 - INFO - Epoch 40/500, Loss: 0.0149\n",
      "2025-08-25 14:06:32,814 - INFO - Epoch 41/500, Loss: 0.0150\n",
      "2025-08-25 14:06:32,903 - INFO - Epoch 42/500, Loss: 0.0149\n",
      "2025-08-25 14:06:32,992 - INFO - Epoch 43/500, Loss: 0.0149\n",
      "2025-08-25 14:06:33,080 - INFO - Epoch 44/500, Loss: 0.0151\n",
      "2025-08-25 14:06:33,169 - INFO - Epoch 45/500, Loss: 0.0148\n",
      "2025-08-25 14:06:33,254 - INFO - Epoch 46/500, Loss: 0.0150\n",
      "2025-08-25 14:06:33,345 - INFO - Epoch 47/500, Loss: 0.0150\n",
      "2025-08-25 14:06:33,435 - INFO - Epoch 48/500, Loss: 0.0149\n",
      "2025-08-25 14:06:33,523 - INFO - Epoch 49/500, Loss: 0.0148\n",
      "2025-08-25 14:06:33,628 - INFO - Epoch 50/500, Loss: 0.0149\n",
      "2025-08-25 14:06:33,721 - INFO - Epoch 51/500, Loss: 0.0150\n",
      "2025-08-25 14:06:33,812 - INFO - Epoch 52/500, Loss: 0.0149\n",
      "2025-08-25 14:06:33,900 - INFO - Epoch 53/500, Loss: 0.0150\n",
      "2025-08-25 14:06:33,989 - INFO - Epoch 54/500, Loss: 0.0151\n",
      "2025-08-25 14:06:34,081 - INFO - Epoch 55/500, Loss: 0.0149\n",
      "2025-08-25 14:06:34,170 - INFO - Epoch 56/500, Loss: 0.0150\n",
      "2025-08-25 14:06:34,258 - INFO - Epoch 57/500, Loss: 0.0149\n",
      "2025-08-25 14:06:34,346 - INFO - Epoch 58/500, Loss: 0.0149\n",
      "2025-08-25 14:06:34,435 - INFO - Epoch 59/500, Loss: 0.0148\n",
      "2025-08-25 14:06:34,522 - INFO - Epoch 60/500, Loss: 0.0149\n",
      "2025-08-25 14:06:34,609 - INFO - Epoch 61/500, Loss: 0.0149\n",
      "2025-08-25 14:06:34,695 - INFO - Epoch 62/500, Loss: 0.0150\n",
      "2025-08-25 14:06:34,782 - INFO - Epoch 63/500, Loss: 0.0150\n",
      "2025-08-25 14:06:34,868 - INFO - Epoch 64/500, Loss: 0.0150\n",
      "2025-08-25 14:06:34,953 - INFO - Epoch 65/500, Loss: 0.0149\n",
      "2025-08-25 14:06:35,039 - INFO - Epoch 66/500, Loss: 0.0149\n",
      "2025-08-25 14:06:35,125 - INFO - Epoch 67/500, Loss: 0.0150\n",
      "2025-08-25 14:06:35,211 - INFO - Epoch 68/500, Loss: 0.0150\n",
      "2025-08-25 14:06:35,300 - INFO - Epoch 69/500, Loss: 0.0148\n",
      "2025-08-25 14:06:35,388 - INFO - Epoch 70/500, Loss: 0.0148\n",
      "2025-08-25 14:06:35,476 - INFO - Epoch 71/500, Loss: 0.0150\n",
      "2025-08-25 14:06:35,565 - INFO - Epoch 72/500, Loss: 0.0149\n",
      "2025-08-25 14:06:35,650 - INFO - Epoch 73/500, Loss: 0.0150\n",
      "2025-08-25 14:06:35,737 - INFO - Epoch 74/500, Loss: 0.0150\n",
      "2025-08-25 14:06:35,824 - INFO - Epoch 75/500, Loss: 0.0149\n",
      "2025-08-25 14:06:35,914 - INFO - Epoch 76/500, Loss: 0.0150\n",
      "2025-08-25 14:06:36,001 - INFO - Epoch 77/500, Loss: 0.0151\n",
      "2025-08-25 14:06:36,089 - INFO - Epoch 78/500, Loss: 0.0150\n",
      "2025-08-25 14:06:36,176 - INFO - Epoch 79/500, Loss: 0.0149\n",
      "2025-08-25 14:06:36,262 - INFO - Epoch 80/500, Loss: 0.0149\n",
      "2025-08-25 14:06:36,349 - INFO - Epoch 81/500, Loss: 0.0149\n",
      "2025-08-25 14:06:36,436 - INFO - Epoch 82/500, Loss: 0.0150\n",
      "2025-08-25 14:06:36,522 - INFO - Epoch 83/500, Loss: 0.0148\n",
      "2025-08-25 14:06:36,608 - INFO - Epoch 84/500, Loss: 0.0148\n",
      "2025-08-25 14:06:36,696 - INFO - Epoch 85/500, Loss: 0.0150\n",
      "2025-08-25 14:06:36,786 - INFO - Epoch 86/500, Loss: 0.0151\n",
      "2025-08-25 14:06:36,874 - INFO - Epoch 87/500, Loss: 0.0149\n",
      "2025-08-25 14:06:36,962 - INFO - Epoch 88/500, Loss: 0.0150\n",
      "2025-08-25 14:06:37,058 - INFO - Epoch 89/500, Loss: 0.0150\n",
      "2025-08-25 14:06:37,151 - INFO - Epoch 90/500, Loss: 0.0148\n",
      "2025-08-25 14:06:37,241 - INFO - Epoch 91/500, Loss: 0.0148\n",
      "2025-08-25 14:06:37,329 - INFO - Epoch 92/500, Loss: 0.0149\n",
      "2025-08-25 14:06:37,416 - INFO - Epoch 93/500, Loss: 0.0148\n",
      "2025-08-25 14:06:37,505 - INFO - Epoch 94/500, Loss: 0.0149\n",
      "2025-08-25 14:06:37,592 - INFO - Epoch 95/500, Loss: 0.0148\n",
      "2025-08-25 14:06:37,679 - INFO - Epoch 96/500, Loss: 0.0150\n",
      "2025-08-25 14:06:37,764 - INFO - Epoch 97/500, Loss: 0.0148\n",
      "2025-08-25 14:06:37,850 - INFO - Epoch 98/500, Loss: 0.0148\n",
      "2025-08-25 14:06:37,937 - INFO - Epoch 99/500, Loss: 0.0149\n",
      "2025-08-25 14:06:38,023 - INFO - Epoch 100/500, Loss: 0.0150\n",
      "2025-08-25 14:06:38,109 - INFO - Epoch 101/500, Loss: 0.0150\n",
      "2025-08-25 14:06:38,196 - INFO - Epoch 102/500, Loss: 0.0148\n",
      "2025-08-25 14:06:38,284 - INFO - Epoch 103/500, Loss: 0.0149\n",
      "2025-08-25 14:06:38,370 - INFO - Epoch 104/500, Loss: 0.0149\n",
      "2025-08-25 14:06:38,456 - INFO - Epoch 105/500, Loss: 0.0148\n",
      "2025-08-25 14:06:38,541 - INFO - Epoch 106/500, Loss: 0.0149\n",
      "2025-08-25 14:06:38,625 - INFO - Epoch 107/500, Loss: 0.0150\n",
      "2025-08-25 14:06:38,711 - INFO - Epoch 108/500, Loss: 0.0149\n",
      "2025-08-25 14:06:38,797 - INFO - Epoch 109/500, Loss: 0.0149\n",
      "2025-08-25 14:06:38,881 - INFO - Epoch 110/500, Loss: 0.0149\n",
      "2025-08-25 14:06:38,966 - INFO - Epoch 111/500, Loss: 0.0149\n",
      "2025-08-25 14:06:39,052 - INFO - Epoch 112/500, Loss: 0.0148\n",
      "2025-08-25 14:06:39,136 - INFO - Epoch 113/500, Loss: 0.0149\n",
      "2025-08-25 14:06:39,223 - INFO - Epoch 114/500, Loss: 0.0151\n",
      "2025-08-25 14:06:39,310 - INFO - Epoch 115/500, Loss: 0.0149\n",
      "2025-08-25 14:06:39,394 - INFO - Epoch 116/500, Loss: 0.0148\n",
      "2025-08-25 14:06:39,480 - INFO - Epoch 117/500, Loss: 0.0150\n",
      "2025-08-25 14:06:39,565 - INFO - Epoch 118/500, Loss: 0.0149\n",
      "2025-08-25 14:06:39,652 - INFO - Epoch 119/500, Loss: 0.0149\n",
      "2025-08-25 14:06:39,739 - INFO - Epoch 120/500, Loss: 0.0148\n",
      "2025-08-25 14:06:39,827 - INFO - Epoch 121/500, Loss: 0.0149\n",
      "2025-08-25 14:06:39,915 - INFO - Epoch 122/500, Loss: 0.0149\n",
      "2025-08-25 14:06:40,004 - INFO - Epoch 123/500, Loss: 0.0152\n",
      "2025-08-25 14:06:40,091 - INFO - Epoch 124/500, Loss: 0.0149\n",
      "2025-08-25 14:06:40,178 - INFO - Epoch 125/500, Loss: 0.0151\n",
      "2025-08-25 14:06:40,265 - INFO - Epoch 126/500, Loss: 0.0148\n",
      "2025-08-25 14:06:40,352 - INFO - Epoch 127/500, Loss: 0.0150\n",
      "2025-08-25 14:06:40,453 - INFO - Epoch 128/500, Loss: 0.0148\n",
      "2025-08-25 14:06:40,539 - INFO - Epoch 129/500, Loss: 0.0148\n",
      "2025-08-25 14:06:40,627 - INFO - Epoch 130/500, Loss: 0.0149\n",
      "2025-08-25 14:06:40,713 - INFO - Epoch 131/500, Loss: 0.0148\n",
      "2025-08-25 14:06:40,801 - INFO - Epoch 132/500, Loss: 0.0149\n",
      "2025-08-25 14:06:40,890 - INFO - Epoch 133/500, Loss: 0.0148\n",
      "2025-08-25 14:06:40,980 - INFO - Epoch 134/500, Loss: 0.0149\n",
      "2025-08-25 14:06:41,068 - INFO - Epoch 135/500, Loss: 0.0149\n",
      "2025-08-25 14:06:41,156 - INFO - Epoch 136/500, Loss: 0.0149\n",
      "2025-08-25 14:06:41,247 - INFO - Epoch 137/500, Loss: 0.0148\n",
      "2025-08-25 14:06:41,335 - INFO - Epoch 138/500, Loss: 0.0148\n",
      "2025-08-25 14:06:41,438 - INFO - Epoch 139/500, Loss: 0.0149\n",
      "2025-08-25 14:06:41,525 - INFO - Epoch 140/500, Loss: 0.0148\n",
      "2025-08-25 14:06:41,611 - INFO - Epoch 141/500, Loss: 0.0150\n",
      "2025-08-25 14:06:41,700 - INFO - Epoch 142/500, Loss: 0.0153\n",
      "2025-08-25 14:06:41,786 - INFO - Epoch 143/500, Loss: 0.0149\n",
      "2025-08-25 14:06:41,871 - INFO - Epoch 144/500, Loss: 0.0149\n",
      "2025-08-25 14:06:41,961 - INFO - Epoch 145/500, Loss: 0.0149\n",
      "2025-08-25 14:06:42,049 - INFO - Epoch 146/500, Loss: 0.0148\n",
      "2025-08-25 14:06:42,136 - INFO - Epoch 147/500, Loss: 0.0151\n",
      "2025-08-25 14:06:42,224 - INFO - Epoch 148/500, Loss: 0.0149\n",
      "2025-08-25 14:06:42,310 - INFO - Epoch 149/500, Loss: 0.0149\n",
      "2025-08-25 14:06:42,400 - INFO - Epoch 150/500, Loss: 0.0149\n",
      "2025-08-25 14:06:42,487 - INFO - Epoch 151/500, Loss: 0.0150\n",
      "2025-08-25 14:06:42,574 - INFO - Epoch 152/500, Loss: 0.0150\n",
      "2025-08-25 14:06:42,662 - INFO - Epoch 153/500, Loss: 0.0150\n",
      "2025-08-25 14:06:42,749 - INFO - Epoch 154/500, Loss: 0.0152\n",
      "2025-08-25 14:06:42,835 - INFO - Epoch 155/500, Loss: 0.0149\n",
      "2025-08-25 14:06:42,924 - INFO - Epoch 156/500, Loss: 0.0149\n",
      "2025-08-25 14:06:43,015 - INFO - Epoch 157/500, Loss: 0.0151\n",
      "2025-08-25 14:06:43,119 - INFO - Epoch 158/500, Loss: 0.0150\n",
      "2025-08-25 14:06:43,210 - INFO - Epoch 159/500, Loss: 0.0148\n",
      "2025-08-25 14:06:43,302 - INFO - Epoch 160/500, Loss: 0.0149\n",
      "2025-08-25 14:06:43,395 - INFO - Epoch 161/500, Loss: 0.0150\n",
      "2025-08-25 14:06:43,484 - INFO - Epoch 162/500, Loss: 0.0149\n",
      "2025-08-25 14:06:43,574 - INFO - Epoch 163/500, Loss: 0.0151\n",
      "2025-08-25 14:06:43,664 - INFO - Epoch 164/500, Loss: 0.0151\n",
      "2025-08-25 14:06:43,754 - INFO - Epoch 165/500, Loss: 0.0148\n",
      "2025-08-25 14:06:43,858 - INFO - Epoch 166/500, Loss: 0.0148\n",
      "2025-08-25 14:06:43,950 - INFO - Epoch 167/500, Loss: 0.0151\n",
      "2025-08-25 14:06:44,041 - INFO - Epoch 168/500, Loss: 0.0150\n",
      "2025-08-25 14:06:44,131 - INFO - Epoch 169/500, Loss: 0.0150\n",
      "2025-08-25 14:06:44,219 - INFO - Epoch 170/500, Loss: 0.0150\n",
      "2025-08-25 14:06:44,308 - INFO - Epoch 171/500, Loss: 0.0150\n",
      "2025-08-25 14:06:44,399 - INFO - Epoch 172/500, Loss: 0.0147\n",
      "2025-08-25 14:06:44,489 - INFO - Epoch 173/500, Loss: 0.0148\n",
      "2025-08-25 14:06:44,580 - INFO - Epoch 174/500, Loss: 0.0153\n",
      "2025-08-25 14:06:44,668 - INFO - Epoch 175/500, Loss: 0.0149\n",
      "2025-08-25 14:06:44,758 - INFO - Epoch 176/500, Loss: 0.0151\n",
      "2025-08-25 14:06:44,845 - INFO - Epoch 177/500, Loss: 0.0149\n",
      "2025-08-25 14:06:44,931 - INFO - Epoch 178/500, Loss: 0.0149\n",
      "2025-08-25 14:06:45,015 - INFO - Epoch 179/500, Loss: 0.0149\n",
      "2025-08-25 14:06:45,102 - INFO - Epoch 180/500, Loss: 0.0150\n",
      "2025-08-25 14:06:45,187 - INFO - Epoch 181/500, Loss: 0.0149\n",
      "2025-08-25 14:06:45,275 - INFO - Epoch 182/500, Loss: 0.0149\n",
      "2025-08-25 14:06:45,362 - INFO - Epoch 183/500, Loss: 0.0150\n",
      "2025-08-25 14:06:45,449 - INFO - Epoch 184/500, Loss: 0.0149\n",
      "2025-08-25 14:06:45,536 - INFO - Epoch 185/500, Loss: 0.0149\n",
      "2025-08-25 14:06:45,624 - INFO - Epoch 186/500, Loss: 0.0149\n",
      "2025-08-25 14:06:45,713 - INFO - Epoch 187/500, Loss: 0.0150\n",
      "2025-08-25 14:06:45,798 - INFO - Epoch 188/500, Loss: 0.0150\n",
      "2025-08-25 14:06:45,884 - INFO - Epoch 189/500, Loss: 0.0149\n",
      "2025-08-25 14:06:45,969 - INFO - Epoch 190/500, Loss: 0.0148\n",
      "2025-08-25 14:06:46,058 - INFO - Epoch 191/500, Loss: 0.0148\n",
      "2025-08-25 14:06:46,146 - INFO - Epoch 192/500, Loss: 0.0148\n",
      "2025-08-25 14:06:46,232 - INFO - Epoch 193/500, Loss: 0.0150\n",
      "2025-08-25 14:06:46,318 - INFO - Epoch 194/500, Loss: 0.0148\n",
      "2025-08-25 14:06:46,403 - INFO - Epoch 195/500, Loss: 0.0148\n",
      "2025-08-25 14:06:46,489 - INFO - Epoch 196/500, Loss: 0.0148\n",
      "2025-08-25 14:06:46,576 - INFO - Epoch 197/500, Loss: 0.0147\n",
      "2025-08-25 14:06:46,663 - INFO - Epoch 198/500, Loss: 0.0148\n",
      "2025-08-25 14:06:46,749 - INFO - Epoch 199/500, Loss: 0.0150\n",
      "2025-08-25 14:06:46,835 - INFO - Epoch 200/500, Loss: 0.0147\n",
      "2025-08-25 14:06:46,921 - INFO - Epoch 201/500, Loss: 0.0150\n",
      "2025-08-25 14:06:47,009 - INFO - Epoch 202/500, Loss: 0.0148\n",
      "2025-08-25 14:06:47,094 - INFO - Epoch 203/500, Loss: 0.0150\n",
      "2025-08-25 14:06:47,180 - INFO - Epoch 204/500, Loss: 0.0149\n",
      "2025-08-25 14:06:47,266 - INFO - Epoch 205/500, Loss: 0.0149\n",
      "2025-08-25 14:06:47,352 - INFO - Epoch 206/500, Loss: 0.0151\n",
      "2025-08-25 14:06:47,438 - INFO - Epoch 207/500, Loss: 0.0151\n",
      "2025-08-25 14:06:47,523 - INFO - Epoch 208/500, Loss: 0.0149\n",
      "2025-08-25 14:06:47,610 - INFO - Epoch 209/500, Loss: 0.0149\n",
      "2025-08-25 14:06:47,695 - INFO - Epoch 210/500, Loss: 0.0149\n",
      "2025-08-25 14:06:47,781 - INFO - Epoch 211/500, Loss: 0.0147\n",
      "2025-08-25 14:06:47,865 - INFO - Epoch 212/500, Loss: 0.0150\n",
      "2025-08-25 14:06:47,950 - INFO - Epoch 213/500, Loss: 0.0148\n",
      "2025-08-25 14:06:48,036 - INFO - Epoch 214/500, Loss: 0.0151\n",
      "2025-08-25 14:06:48,136 - INFO - Epoch 215/500, Loss: 0.0150\n",
      "2025-08-25 14:06:48,227 - INFO - Epoch 216/500, Loss: 0.0149\n",
      "2025-08-25 14:06:48,313 - INFO - Epoch 217/500, Loss: 0.0149\n",
      "2025-08-25 14:06:48,399 - INFO - Epoch 218/500, Loss: 0.0148\n",
      "2025-08-25 14:06:48,484 - INFO - Epoch 219/500, Loss: 0.0152\n",
      "2025-08-25 14:06:48,571 - INFO - Epoch 220/500, Loss: 0.0151\n",
      "2025-08-25 14:06:48,658 - INFO - Epoch 221/500, Loss: 0.0151\n",
      "2025-08-25 14:06:48,744 - INFO - Epoch 222/500, Loss: 0.0148\n",
      "2025-08-25 14:06:48,830 - INFO - Epoch 223/500, Loss: 0.0150\n",
      "2025-08-25 14:06:48,916 - INFO - Epoch 224/500, Loss: 0.0148\n",
      "2025-08-25 14:06:49,017 - INFO - Epoch 225/500, Loss: 0.0149\n",
      "2025-08-25 14:06:49,104 - INFO - Epoch 226/500, Loss: 0.0148\n",
      "2025-08-25 14:06:49,193 - INFO - Epoch 227/500, Loss: 0.0149\n",
      "2025-08-25 14:06:49,279 - INFO - Epoch 228/500, Loss: 0.0147\n",
      "2025-08-25 14:06:49,365 - INFO - Epoch 229/500, Loss: 0.0150\n",
      "2025-08-25 14:06:49,453 - INFO - Epoch 230/500, Loss: 0.0150\n",
      "2025-08-25 14:06:49,539 - INFO - Epoch 231/500, Loss: 0.0151\n",
      "2025-08-25 14:06:49,625 - INFO - Epoch 232/500, Loss: 0.0149\n",
      "2025-08-25 14:06:49,711 - INFO - Epoch 233/500, Loss: 0.0151\n",
      "2025-08-25 14:06:49,796 - INFO - Epoch 234/500, Loss: 0.0151\n",
      "2025-08-25 14:06:49,880 - INFO - Epoch 235/500, Loss: 0.0151\n",
      "2025-08-25 14:06:49,967 - INFO - Epoch 236/500, Loss: 0.0152\n",
      "2025-08-25 14:06:50,053 - INFO - Epoch 237/500, Loss: 0.0150\n",
      "2025-08-25 14:06:50,140 - INFO - Epoch 238/500, Loss: 0.0149\n",
      "2025-08-25 14:06:50,226 - INFO - Epoch 239/500, Loss: 0.0150\n",
      "2025-08-25 14:06:50,312 - INFO - Epoch 240/500, Loss: 0.0149\n",
      "2025-08-25 14:06:50,399 - INFO - Epoch 241/500, Loss: 0.0149\n",
      "2025-08-25 14:06:50,487 - INFO - Epoch 242/500, Loss: 0.0148\n",
      "2025-08-25 14:06:50,575 - INFO - Epoch 243/500, Loss: 0.0150\n",
      "2025-08-25 14:06:50,661 - INFO - Epoch 244/500, Loss: 0.0148\n",
      "2025-08-25 14:06:50,746 - INFO - Epoch 245/500, Loss: 0.0148\n",
      "2025-08-25 14:06:50,833 - INFO - Epoch 246/500, Loss: 0.0149\n",
      "2025-08-25 14:06:50,919 - INFO - Epoch 247/500, Loss: 0.0150\n",
      "2025-08-25 14:06:51,006 - INFO - Epoch 248/500, Loss: 0.0149\n",
      "2025-08-25 14:06:51,092 - INFO - Epoch 249/500, Loss: 0.0148\n",
      "2025-08-25 14:06:51,176 - INFO - Epoch 250/500, Loss: 0.0149\n",
      "2025-08-25 14:06:51,262 - INFO - Epoch 251/500, Loss: 0.0148\n",
      "2025-08-25 14:06:51,348 - INFO - Epoch 252/500, Loss: 0.0148\n",
      "2025-08-25 14:06:51,434 - INFO - Epoch 253/500, Loss: 0.0152\n",
      "2025-08-25 14:06:51,520 - INFO - Epoch 254/500, Loss: 0.0151\n",
      "2025-08-25 14:06:51,605 - INFO - Epoch 255/500, Loss: 0.0151\n",
      "2025-08-25 14:06:51,691 - INFO - Epoch 256/500, Loss: 0.0150\n",
      "2025-08-25 14:06:51,778 - INFO - Epoch 257/500, Loss: 0.0150\n",
      "2025-08-25 14:06:51,862 - INFO - Epoch 258/500, Loss: 0.0149\n",
      "2025-08-25 14:06:51,947 - INFO - Epoch 259/500, Loss: 0.0149\n",
      "2025-08-25 14:06:52,033 - INFO - Epoch 260/500, Loss: 0.0148\n",
      "2025-08-25 14:06:52,119 - INFO - Epoch 261/500, Loss: 0.0151\n",
      "2025-08-25 14:06:52,207 - INFO - Epoch 262/500, Loss: 0.0151\n",
      "2025-08-25 14:06:52,297 - INFO - Epoch 263/500, Loss: 0.0152\n",
      "2025-08-25 14:06:52,385 - INFO - Epoch 264/500, Loss: 0.0152\n",
      "2025-08-25 14:06:52,476 - INFO - Epoch 265/500, Loss: 0.0150\n",
      "2025-08-25 14:06:52,566 - INFO - Epoch 266/500, Loss: 0.0148\n",
      "2025-08-25 14:06:52,654 - INFO - Epoch 267/500, Loss: 0.0149\n",
      "2025-08-25 14:06:52,742 - INFO - Epoch 268/500, Loss: 0.0148\n",
      "2025-08-25 14:06:52,833 - INFO - Epoch 269/500, Loss: 0.0149\n",
      "2025-08-25 14:06:52,944 - INFO - Epoch 270/500, Loss: 0.0148\n",
      "2025-08-25 14:06:53,036 - INFO - Epoch 271/500, Loss: 0.0152\n",
      "2025-08-25 14:06:53,124 - INFO - Epoch 272/500, Loss: 0.0149\n",
      "2025-08-25 14:06:53,213 - INFO - Epoch 273/500, Loss: 0.0149\n",
      "2025-08-25 14:06:53,300 - INFO - Epoch 274/500, Loss: 0.0151\n",
      "2025-08-25 14:06:53,389 - INFO - Epoch 275/500, Loss: 0.0149\n",
      "2025-08-25 14:06:53,481 - INFO - Epoch 276/500, Loss: 0.0150\n",
      "2025-08-25 14:06:53,572 - INFO - Epoch 277/500, Loss: 0.0149\n",
      "2025-08-25 14:06:53,665 - INFO - Epoch 278/500, Loss: 0.0151\n",
      "2025-08-25 14:06:53,755 - INFO - Epoch 279/500, Loss: 0.0149\n",
      "2025-08-25 14:06:53,843 - INFO - Epoch 280/500, Loss: 0.0148\n",
      "2025-08-25 14:06:53,932 - INFO - Epoch 281/500, Loss: 0.0149\n",
      "2025-08-25 14:06:54,024 - INFO - Epoch 282/500, Loss: 0.0149\n",
      "2025-08-25 14:06:54,113 - INFO - Epoch 283/500, Loss: 0.0150\n",
      "2025-08-25 14:06:54,200 - INFO - Epoch 284/500, Loss: 0.0149\n",
      "2025-08-25 14:06:54,294 - INFO - Epoch 285/500, Loss: 0.0148\n",
      "2025-08-25 14:06:54,385 - INFO - Epoch 286/500, Loss: 0.0148\n",
      "2025-08-25 14:06:54,474 - INFO - Epoch 287/500, Loss: 0.0148\n",
      "2025-08-25 14:06:54,563 - INFO - Epoch 288/500, Loss: 0.0150\n",
      "2025-08-25 14:06:54,650 - INFO - Epoch 289/500, Loss: 0.0150\n",
      "2025-08-25 14:06:54,737 - INFO - Epoch 290/500, Loss: 0.0148\n",
      "2025-08-25 14:06:54,826 - INFO - Epoch 291/500, Loss: 0.0150\n",
      "2025-08-25 14:06:54,916 - INFO - Epoch 292/500, Loss: 0.0149\n",
      "2025-08-25 14:06:55,005 - INFO - Epoch 293/500, Loss: 0.0148\n",
      "2025-08-25 14:06:55,094 - INFO - Epoch 294/500, Loss: 0.0150\n",
      "2025-08-25 14:06:55,181 - INFO - Epoch 295/500, Loss: 0.0151\n",
      "2025-08-25 14:06:55,269 - INFO - Epoch 296/500, Loss: 0.0155\n",
      "2025-08-25 14:06:55,357 - INFO - Epoch 297/500, Loss: 0.0151\n",
      "2025-08-25 14:06:55,449 - INFO - Epoch 298/500, Loss: 0.0149\n",
      "2025-08-25 14:06:55,537 - INFO - Epoch 299/500, Loss: 0.0154\n",
      "2025-08-25 14:06:55,625 - INFO - Epoch 300/500, Loss: 0.0149\n",
      "2025-08-25 14:06:55,714 - INFO - Epoch 301/500, Loss: 0.0149\n",
      "2025-08-25 14:06:55,803 - INFO - Epoch 302/500, Loss: 0.0149\n",
      "2025-08-25 14:06:55,893 - INFO - Epoch 303/500, Loss: 0.0150\n",
      "2025-08-25 14:06:55,981 - INFO - Epoch 304/500, Loss: 0.0150\n",
      "2025-08-25 14:06:56,069 - INFO - Epoch 305/500, Loss: 0.0148\n",
      "2025-08-25 14:06:56,160 - INFO - Epoch 306/500, Loss: 0.0149\n",
      "2025-08-25 14:06:56,248 - INFO - Epoch 307/500, Loss: 0.0149\n",
      "2025-08-25 14:06:56,335 - INFO - Epoch 308/500, Loss: 0.0151\n",
      "2025-08-25 14:06:56,422 - INFO - Epoch 309/500, Loss: 0.0149\n",
      "2025-08-25 14:06:56,510 - INFO - Epoch 310/500, Loss: 0.0148\n",
      "2025-08-25 14:06:56,598 - INFO - Epoch 311/500, Loss: 0.0149\n",
      "2025-08-25 14:06:56,686 - INFO - Epoch 312/500, Loss: 0.0148\n",
      "2025-08-25 14:06:56,777 - INFO - Epoch 313/500, Loss: 0.0150\n",
      "2025-08-25 14:06:56,868 - INFO - Epoch 314/500, Loss: 0.0149\n",
      "2025-08-25 14:06:56,960 - INFO - Epoch 315/500, Loss: 0.0151\n",
      "2025-08-25 14:06:57,050 - INFO - Epoch 316/500, Loss: 0.0149\n",
      "2025-08-25 14:06:57,138 - INFO - Epoch 317/500, Loss: 0.0149\n",
      "2025-08-25 14:06:57,229 - INFO - Epoch 318/500, Loss: 0.0152\n",
      "2025-08-25 14:06:57,316 - INFO - Epoch 319/500, Loss: 0.0149\n",
      "2025-08-25 14:06:57,419 - INFO - Epoch 320/500, Loss: 0.0148\n",
      "2025-08-25 14:06:57,508 - INFO - Epoch 321/500, Loss: 0.0149\n",
      "2025-08-25 14:06:57,597 - INFO - Epoch 322/500, Loss: 0.0149\n",
      "2025-08-25 14:06:57,684 - INFO - Epoch 323/500, Loss: 0.0151\n",
      "2025-08-25 14:06:57,777 - INFO - Epoch 324/500, Loss: 0.0150\n",
      "2025-08-25 14:06:57,884 - INFO - Epoch 325/500, Loss: 0.0148\n",
      "2025-08-25 14:06:57,974 - INFO - Epoch 326/500, Loss: 0.0150\n",
      "2025-08-25 14:06:58,065 - INFO - Epoch 327/500, Loss: 0.0153\n",
      "2025-08-25 14:06:58,154 - INFO - Epoch 328/500, Loss: 0.0151\n",
      "2025-08-25 14:06:58,245 - INFO - Epoch 329/500, Loss: 0.0150\n",
      "2025-08-25 14:06:58,335 - INFO - Epoch 330/500, Loss: 0.0150\n",
      "2025-08-25 14:06:58,424 - INFO - Epoch 331/500, Loss: 0.0150\n",
      "2025-08-25 14:06:58,517 - INFO - Epoch 332/500, Loss: 0.0152\n",
      "2025-08-25 14:06:58,614 - INFO - Epoch 333/500, Loss: 0.0150\n",
      "2025-08-25 14:06:58,708 - INFO - Epoch 334/500, Loss: 0.0148\n",
      "2025-08-25 14:06:58,800 - INFO - Epoch 335/500, Loss: 0.0148\n",
      "2025-08-25 14:06:58,887 - INFO - Epoch 336/500, Loss: 0.0148\n",
      "2025-08-25 14:06:58,975 - INFO - Epoch 337/500, Loss: 0.0150\n",
      "2025-08-25 14:06:59,063 - INFO - Epoch 338/500, Loss: 0.0148\n",
      "2025-08-25 14:06:59,151 - INFO - Epoch 339/500, Loss: 0.0150\n",
      "2025-08-25 14:06:59,240 - INFO - Epoch 340/500, Loss: 0.0148\n",
      "2025-08-25 14:06:59,330 - INFO - Epoch 341/500, Loss: 0.0148\n",
      "2025-08-25 14:06:59,421 - INFO - Epoch 342/500, Loss: 0.0150\n",
      "2025-08-25 14:06:59,508 - INFO - Epoch 343/500, Loss: 0.0148\n",
      "2025-08-25 14:06:59,596 - INFO - Epoch 344/500, Loss: 0.0149\n",
      "2025-08-25 14:06:59,684 - INFO - Epoch 345/500, Loss: 0.0149\n",
      "2025-08-25 14:06:59,769 - INFO - Epoch 346/500, Loss: 0.0151\n",
      "2025-08-25 14:06:59,855 - INFO - Epoch 347/500, Loss: 0.0150\n",
      "2025-08-25 14:06:59,941 - INFO - Epoch 348/500, Loss: 0.0151\n",
      "2025-08-25 14:07:00,029 - INFO - Epoch 349/500, Loss: 0.0149\n",
      "2025-08-25 14:07:00,117 - INFO - Epoch 350/500, Loss: 0.0148\n",
      "2025-08-25 14:07:00,204 - INFO - Epoch 351/500, Loss: 0.0148\n",
      "2025-08-25 14:07:00,290 - INFO - Epoch 352/500, Loss: 0.0149\n",
      "2025-08-25 14:07:00,375 - INFO - Epoch 353/500, Loss: 0.0149\n",
      "2025-08-25 14:07:00,462 - INFO - Epoch 354/500, Loss: 0.0149\n",
      "2025-08-25 14:07:00,551 - INFO - Epoch 355/500, Loss: 0.0150\n",
      "2025-08-25 14:07:00,637 - INFO - Epoch 356/500, Loss: 0.0149\n",
      "2025-08-25 14:07:00,722 - INFO - Epoch 357/500, Loss: 0.0148\n",
      "2025-08-25 14:07:00,807 - INFO - Epoch 358/500, Loss: 0.0150\n",
      "2025-08-25 14:07:00,892 - INFO - Epoch 359/500, Loss: 0.0150\n",
      "2025-08-25 14:07:00,977 - INFO - Epoch 360/500, Loss: 0.0150\n",
      "2025-08-25 14:07:01,063 - INFO - Epoch 361/500, Loss: 0.0151\n",
      "2025-08-25 14:07:01,149 - INFO - Epoch 362/500, Loss: 0.0149\n",
      "2025-08-25 14:07:01,235 - INFO - Epoch 363/500, Loss: 0.0148\n",
      "2025-08-25 14:07:01,320 - INFO - Epoch 364/500, Loss: 0.0149\n",
      "2025-08-25 14:07:01,405 - INFO - Epoch 365/500, Loss: 0.0147\n",
      "2025-08-25 14:07:01,493 - INFO - Epoch 366/500, Loss: 0.0149\n",
      "2025-08-25 14:07:01,580 - INFO - Epoch 367/500, Loss: 0.0149\n",
      "2025-08-25 14:07:01,669 - INFO - Epoch 368/500, Loss: 0.0148\n",
      "2025-08-25 14:07:01,755 - INFO - Epoch 369/500, Loss: 0.0150\n",
      "2025-08-25 14:07:01,842 - INFO - Epoch 370/500, Loss: 0.0150\n",
      "2025-08-25 14:07:01,929 - INFO - Epoch 371/500, Loss: 0.0153\n",
      "2025-08-25 14:07:02,015 - INFO - Epoch 372/500, Loss: 0.0154\n",
      "2025-08-25 14:07:02,113 - INFO - Epoch 373/500, Loss: 0.0151\n",
      "2025-08-25 14:07:02,198 - INFO - Epoch 374/500, Loss: 0.0150\n",
      "2025-08-25 14:07:02,283 - INFO - Epoch 375/500, Loss: 0.0148\n",
      "2025-08-25 14:07:02,369 - INFO - Epoch 376/500, Loss: 0.0150\n",
      "2025-08-25 14:07:02,454 - INFO - Epoch 377/500, Loss: 0.0150\n",
      "2025-08-25 14:07:02,540 - INFO - Epoch 378/500, Loss: 0.0148\n",
      "2025-08-25 14:07:02,628 - INFO - Epoch 379/500, Loss: 0.0150\n",
      "2025-08-25 14:07:02,714 - INFO - Epoch 380/500, Loss: 0.0148\n",
      "2025-08-25 14:07:02,800 - INFO - Epoch 381/500, Loss: 0.0148\n",
      "2025-08-25 14:07:02,886 - INFO - Epoch 382/500, Loss: 0.0149\n",
      "2025-08-25 14:07:02,972 - INFO - Epoch 383/500, Loss: 0.0151\n",
      "2025-08-25 14:07:03,058 - INFO - Epoch 384/500, Loss: 0.0151\n",
      "2025-08-25 14:07:03,144 - INFO - Epoch 385/500, Loss: 0.0151\n",
      "2025-08-25 14:07:03,230 - INFO - Epoch 386/500, Loss: 0.0148\n",
      "2025-08-25 14:07:03,315 - INFO - Epoch 387/500, Loss: 0.0150\n",
      "2025-08-25 14:07:03,401 - INFO - Epoch 388/500, Loss: 0.0148\n",
      "2025-08-25 14:07:03,487 - INFO - Epoch 389/500, Loss: 0.0150\n",
      "2025-08-25 14:07:03,572 - INFO - Epoch 390/500, Loss: 0.0148\n",
      "2025-08-25 14:07:03,659 - INFO - Epoch 391/500, Loss: 0.0149\n",
      "2025-08-25 14:07:03,746 - INFO - Epoch 392/500, Loss: 0.0149\n",
      "2025-08-25 14:07:03,831 - INFO - Epoch 393/500, Loss: 0.0149\n",
      "2025-08-25 14:07:03,918 - INFO - Epoch 394/500, Loss: 0.0149\n",
      "2025-08-25 14:07:04,004 - INFO - Epoch 395/500, Loss: 0.0148\n",
      "2025-08-25 14:07:04,089 - INFO - Epoch 396/500, Loss: 0.0152\n",
      "2025-08-25 14:07:04,177 - INFO - Epoch 397/500, Loss: 0.0152\n",
      "2025-08-25 14:07:04,264 - INFO - Epoch 398/500, Loss: 0.0150\n",
      "2025-08-25 14:07:04,351 - INFO - Epoch 399/500, Loss: 0.0149\n",
      "2025-08-25 14:07:04,439 - INFO - Epoch 400/500, Loss: 0.0152\n",
      "2025-08-25 14:07:04,529 - INFO - Epoch 401/500, Loss: 0.0148\n",
      "2025-08-25 14:07:04,618 - INFO - Epoch 402/500, Loss: 0.0149\n",
      "2025-08-25 14:07:04,709 - INFO - Epoch 403/500, Loss: 0.0148\n",
      "2025-08-25 14:07:04,799 - INFO - Epoch 404/500, Loss: 0.0148\n",
      "2025-08-25 14:07:04,886 - INFO - Epoch 405/500, Loss: 0.0149\n",
      "2025-08-25 14:07:04,974 - INFO - Epoch 406/500, Loss: 0.0149\n",
      "2025-08-25 14:07:05,063 - INFO - Epoch 407/500, Loss: 0.0148\n",
      "2025-08-25 14:07:05,152 - INFO - Epoch 408/500, Loss: 0.0151\n",
      "2025-08-25 14:07:05,241 - INFO - Epoch 409/500, Loss: 0.0152\n",
      "2025-08-25 14:07:05,341 - INFO - Epoch 410/500, Loss: 0.0149\n",
      "2025-08-25 14:07:05,429 - INFO - Epoch 411/500, Loss: 0.0151\n",
      "2025-08-25 14:07:05,516 - INFO - Epoch 412/500, Loss: 0.0148\n",
      "2025-08-25 14:07:05,604 - INFO - Epoch 413/500, Loss: 0.0148\n",
      "2025-08-25 14:07:05,693 - INFO - Epoch 414/500, Loss: 0.0149\n",
      "2025-08-25 14:07:05,782 - INFO - Epoch 415/500, Loss: 0.0150\n",
      "2025-08-25 14:07:05,870 - INFO - Epoch 416/500, Loss: 0.0149\n",
      "2025-08-25 14:07:05,956 - INFO - Epoch 417/500, Loss: 0.0149\n",
      "2025-08-25 14:07:06,041 - INFO - Epoch 418/500, Loss: 0.0151\n",
      "2025-08-25 14:07:06,127 - INFO - Epoch 419/500, Loss: 0.0150\n",
      "2025-08-25 14:07:06,214 - INFO - Epoch 420/500, Loss: 0.0150\n",
      "2025-08-25 14:07:06,299 - INFO - Epoch 421/500, Loss: 0.0149\n",
      "2025-08-25 14:07:06,384 - INFO - Epoch 422/500, Loss: 0.0148\n",
      "2025-08-25 14:07:06,469 - INFO - Epoch 423/500, Loss: 0.0149\n",
      "2025-08-25 14:07:06,560 - INFO - Epoch 424/500, Loss: 0.0149\n",
      "2025-08-25 14:07:06,649 - INFO - Epoch 425/500, Loss: 0.0149\n",
      "2025-08-25 14:07:06,763 - INFO - Epoch 426/500, Loss: 0.0147\n",
      "2025-08-25 14:07:06,854 - INFO - Epoch 427/500, Loss: 0.0149\n",
      "2025-08-25 14:07:06,942 - INFO - Epoch 428/500, Loss: 0.0150\n",
      "2025-08-25 14:07:07,030 - INFO - Epoch 429/500, Loss: 0.0149\n",
      "2025-08-25 14:07:07,116 - INFO - Epoch 430/500, Loss: 0.0149\n",
      "2025-08-25 14:07:07,204 - INFO - Epoch 431/500, Loss: 0.0149\n",
      "2025-08-25 14:07:07,291 - INFO - Epoch 432/500, Loss: 0.0148\n",
      "2025-08-25 14:07:07,378 - INFO - Epoch 433/500, Loss: 0.0148\n",
      "2025-08-25 14:07:07,467 - INFO - Epoch 434/500, Loss: 0.0148\n",
      "2025-08-25 14:07:07,555 - INFO - Epoch 435/500, Loss: 0.0151\n",
      "2025-08-25 14:07:07,647 - INFO - Epoch 436/500, Loss: 0.0148\n",
      "2025-08-25 14:07:07,740 - INFO - Epoch 437/500, Loss: 0.0149\n",
      "2025-08-25 14:07:07,833 - INFO - Epoch 438/500, Loss: 0.0149\n",
      "2025-08-25 14:07:07,922 - INFO - Epoch 439/500, Loss: 0.0148\n",
      "2025-08-25 14:07:08,012 - INFO - Epoch 440/500, Loss: 0.0149\n",
      "2025-08-25 14:07:08,101 - INFO - Epoch 441/500, Loss: 0.0147\n",
      "2025-08-25 14:07:08,190 - INFO - Epoch 442/500, Loss: 0.0149\n",
      "2025-08-25 14:07:08,279 - INFO - Epoch 443/500, Loss: 0.0151\n",
      "2025-08-25 14:07:08,367 - INFO - Epoch 444/500, Loss: 0.0150\n",
      "2025-08-25 14:07:08,456 - INFO - Epoch 445/500, Loss: 0.0149\n",
      "2025-08-25 14:07:08,546 - INFO - Epoch 446/500, Loss: 0.0150\n",
      "2025-08-25 14:07:08,635 - INFO - Epoch 447/500, Loss: 0.0158\n",
      "2025-08-25 14:07:08,723 - INFO - Epoch 448/500, Loss: 0.0154\n",
      "2025-08-25 14:07:08,810 - INFO - Epoch 449/500, Loss: 0.0151\n",
      "2025-08-25 14:07:08,897 - INFO - Epoch 450/500, Loss: 0.0153\n",
      "2025-08-25 14:07:08,984 - INFO - Epoch 451/500, Loss: 0.0149\n",
      "2025-08-25 14:07:09,070 - INFO - Epoch 452/500, Loss: 0.0150\n",
      "2025-08-25 14:07:09,160 - INFO - Epoch 453/500, Loss: 0.0150\n",
      "2025-08-25 14:07:09,249 - INFO - Epoch 454/500, Loss: 0.0149\n",
      "2025-08-25 14:07:09,337 - INFO - Epoch 455/500, Loss: 0.0148\n",
      "2025-08-25 14:07:09,430 - INFO - Epoch 456/500, Loss: 0.0149\n",
      "2025-08-25 14:07:09,520 - INFO - Epoch 457/500, Loss: 0.0151\n",
      "2025-08-25 14:07:09,607 - INFO - Epoch 458/500, Loss: 0.0152\n",
      "2025-08-25 14:07:09,692 - INFO - Epoch 459/500, Loss: 0.0150\n",
      "2025-08-25 14:07:09,780 - INFO - Epoch 460/500, Loss: 0.0150\n",
      "2025-08-25 14:07:09,870 - INFO - Epoch 461/500, Loss: 0.0149\n",
      "2025-08-25 14:07:09,957 - INFO - Epoch 462/500, Loss: 0.0149\n",
      "2025-08-25 14:07:10,053 - INFO - Epoch 463/500, Loss: 0.0148\n",
      "2025-08-25 14:07:10,140 - INFO - Epoch 464/500, Loss: 0.0149\n",
      "2025-08-25 14:07:10,230 - INFO - Epoch 465/500, Loss: 0.0148\n",
      "2025-08-25 14:07:10,320 - INFO - Epoch 466/500, Loss: 0.0150\n",
      "2025-08-25 14:07:10,406 - INFO - Epoch 467/500, Loss: 0.0149\n",
      "2025-08-25 14:07:10,492 - INFO - Epoch 468/500, Loss: 0.0150\n",
      "2025-08-25 14:07:10,578 - INFO - Epoch 469/500, Loss: 0.0148\n",
      "2025-08-25 14:07:10,665 - INFO - Epoch 470/500, Loss: 0.0149\n",
      "2025-08-25 14:07:10,756 - INFO - Epoch 471/500, Loss: 0.0149\n",
      "2025-08-25 14:07:10,842 - INFO - Epoch 472/500, Loss: 0.0149\n",
      "2025-08-25 14:07:10,931 - INFO - Epoch 473/500, Loss: 0.0150\n",
      "2025-08-25 14:07:11,017 - INFO - Epoch 474/500, Loss: 0.0149\n",
      "2025-08-25 14:07:11,103 - INFO - Epoch 475/500, Loss: 0.0150\n",
      "2025-08-25 14:07:11,193 - INFO - Epoch 476/500, Loss: 0.0152\n",
      "2025-08-25 14:07:11,285 - INFO - Epoch 477/500, Loss: 0.0150\n",
      "2025-08-25 14:07:11,371 - INFO - Epoch 478/500, Loss: 0.0150\n",
      "2025-08-25 14:07:11,457 - INFO - Epoch 479/500, Loss: 0.0150\n",
      "2025-08-25 14:07:11,543 - INFO - Epoch 480/500, Loss: 0.0149\n",
      "2025-08-25 14:07:11,629 - INFO - Epoch 481/500, Loss: 0.0147\n",
      "2025-08-25 14:07:11,719 - INFO - Epoch 482/500, Loss: 0.0152\n",
      "2025-08-25 14:07:11,810 - INFO - Epoch 483/500, Loss: 0.0152\n",
      "2025-08-25 14:07:11,895 - INFO - Epoch 484/500, Loss: 0.0149\n",
      "2025-08-25 14:07:11,994 - INFO - Epoch 485/500, Loss: 0.0148\n",
      "2025-08-25 14:07:12,080 - INFO - Epoch 486/500, Loss: 0.0152\n",
      "2025-08-25 14:07:12,166 - INFO - Epoch 487/500, Loss: 0.0153\n",
      "2025-08-25 14:07:12,258 - INFO - Epoch 488/500, Loss: 0.0149\n",
      "2025-08-25 14:07:12,345 - INFO - Epoch 489/500, Loss: 0.0149\n",
      "2025-08-25 14:07:12,431 - INFO - Epoch 490/500, Loss: 0.0152\n",
      "2025-08-25 14:07:12,516 - INFO - Epoch 491/500, Loss: 0.0152\n",
      "2025-08-25 14:07:12,605 - INFO - Epoch 492/500, Loss: 0.0148\n",
      "2025-08-25 14:07:12,692 - INFO - Epoch 493/500, Loss: 0.0149\n",
      "2025-08-25 14:07:12,777 - INFO - Epoch 494/500, Loss: 0.0148\n",
      "2025-08-25 14:07:12,863 - INFO - Epoch 495/500, Loss: 0.0149\n",
      "2025-08-25 14:07:12,948 - INFO - Epoch 496/500, Loss: 0.0149\n",
      "2025-08-25 14:07:13,034 - INFO - Epoch 497/500, Loss: 0.0148\n",
      "2025-08-25 14:07:13,120 - INFO - Epoch 498/500, Loss: 0.0149\n",
      "2025-08-25 14:07:13,206 - INFO - Epoch 499/500, Loss: 0.0149\n",
      "2025-08-25 14:07:13,292 - INFO - Epoch 500/500, Loss: 0.0148\n",
      "2025-08-25 14:07:13,297 - INFO - Finished GNN training. Model saved to models/EdgeGNN_regressor/EdgeGNN_regressor.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, LeakyReLU, Tanh\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, Batch, DataLoader\n",
    "from torch_geometric.nn import EdgeConv, global_mean_pool\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error # Import for evaluation metrics\n",
    "import random\n",
    "\n",
    "# --- Set random seeds ---\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- Configure Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "TARGET_NAME = 'FEV1_FVC'  \n",
    "TARGET_COLS = ['FEV1_FVC']  \n",
    "\n",
    "# --- Control variable for saving/splitting SUBJID ---\n",
    "USE_SUBJID = True  # Set to True to save and split SUBJID, False otherwise\n",
    "\n",
    "# --- Manually Input Best Parameters Here ---\n",
    "best_lr = 0.001\n",
    "best_hidden_channels = 128\n",
    "best_embedding_dim = 32\n",
    "best_num_conv_layers = 3\n",
    "best_mlp1_layers_dims = [32, 32, 32]\n",
    "best_mlp2_layers_dims = [64, 64, 64]\n",
    "best_edgeconv_aggr = 'max'\n",
    "best_use_batchnorm = True\n",
    "best_activation_name = 'relu'\n",
    "best_epochs = 500\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "activation_map = {'relu': ReLU(), 'leaky_relu': LeakyReLU(), 'tanh': Tanh()}\n",
    "best_mlp_activation = activation_map[best_activation_name]\n",
    "\n",
    "\n",
    "# --- Define the Adjusted EdgeGNN Model ---\n",
    "class AirwayEdgeGNN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, output_channels,\n",
    "                 num_conv_layers=3, mlp1_layers=[64, 64], mlp2_layers=[64, 64],\n",
    "                 mlp_activation=ReLU(), edgeconv_aggr='max', use_batchnorm=False):\n",
    "        super(AirwayEdgeGNN, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        mlp1 = []\n",
    "        in_channels = 2 * num_node_features\n",
    "        for h_dim in mlp1_layers:\n",
    "            mlp1.append(Linear(in_channels, h_dim))\n",
    "            if use_batchnorm:\n",
    "                mlp1.append(BatchNorm1d(h_dim))\n",
    "            mlp1.append(mlp_activation)\n",
    "            in_channels = h_dim\n",
    "        self.convs.append(EdgeConv(nn=Sequential(*mlp1), aggr=edgeconv_aggr))\n",
    "        last_out_channels = mlp1_layers[-1] if mlp1_layers else 2 * num_node_features\n",
    "\n",
    "        for _ in range(num_conv_layers - 1):\n",
    "            mlp_intermediate = []\n",
    "            in_channels = 2 * last_out_channels\n",
    "            for h_dim in mlp2_layers:\n",
    "                mlp_intermediate.append(Linear(in_channels, h_dim))\n",
    "                if use_batchnorm:\n",
    "                    mlp_intermediate.append(BatchNorm1d(h_dim))\n",
    "                mlp_intermediate.append(mlp_activation)\n",
    "                in_channels = h_dim\n",
    "            self.convs.append(EdgeConv(nn=Sequential(*mlp_intermediate), aggr=edgeconv_aggr))\n",
    "            last_out_channels = mlp2_layers[-1] if mlp2_layers else 2 * last_out_channels\n",
    "\n",
    "        self.out = Linear(last_out_channels, output_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- Data Loading Functions (same as before) ---\n",
    "def load_graph_from_excel(filepath):\n",
    "    nodes_df = pd.read_excel(filepath, sheet_name='Nodes')\n",
    "    edges_df = pd.read_excel(filepath, sheet_name='Edges')\n",
    "    nodes = nodes_df['node_id'].tolist()\n",
    "    edges = list(zip(edges_df['bp0'], edges_df['bp1']))\n",
    "    node_features = torch.tensor(nodes_df[['x', 'y', 'z']].values, dtype=torch.float)\n",
    "    edge_features = torch.tensor(edges_df[['generation', 'length', 'diameter', 'InArea', 'OutArea', 'InPeri', 'OutPeri', 'WT', 'WA', 'Din', 'Dout', 'Cr']].values, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features)\n",
    "    return data\n",
    "\n",
    "def load_all_graphs_from_folder(folder_path):\n",
    "    graph_files = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]\n",
    "    airway_trees = [load_graph_from_excel(os.path.join(folder_path, f)) for f in graph_files]\n",
    "    return airway_trees\n",
    "\n",
    "def load_feature_names_from_excel(filepath, column_name='FeatureName', sheet_name='Sheet1'):\n",
    "    \"\"\"\n",
    "    Loads feature names from a specified column in an Excel file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(filepath, sheet_name=sheet_name)\n",
    "        if column_name in df.columns:\n",
    "            return df[column_name].tolist()\n",
    "        else:\n",
    "            logger.error(f\"Column '{column_name}' not found in {filepath}\")\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Feature names file not found at {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading feature names from {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Function to train the GNN ---\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "# --- Function to evaluate the GNN and collect predictions ---\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "    filenames = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data)\n",
    "            loss = criterion(out, data.y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            \n",
    "            predictions.extend(out.cpu().numpy().flatten())\n",
    "            ground_truths.extend(data.y.cpu().numpy().flatten())\n",
    "            # Assuming 'filename' attribute is added to Data objects\n",
    "            filenames.extend(data.filename) \n",
    "            \n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    return avg_loss, predictions, ground_truths, filenames\n",
    "\n",
    "\n",
    "# --- Main Execution for Training, Feature Extraction and K-Fold Split ---\n",
    "if __name__ == \"__main__\":\n",
    "    # --- NEW DATA LOADING SECTION ---\n",
    "    # This replaces the old loading of two separate Excel files\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    print(\"Loading airway trees and target...\")\n",
    "    GRAPH_FOLDER = 'data/airways_607'\n",
    "    airway_trees_raw = load_all_graphs_from_folder(GRAPH_FOLDER)\n",
    "    graph_filenames = [f for f in os.listdir(GRAPH_FOLDER) if f.endswith('.xlsx')]\n",
    "    graph_filenames.sort() # Ensure filenames are sorted to match the order of airway_trees_raw if loaded sequentially\n",
    "\n",
    "    # Load the combined data file\n",
    "    COMBINED_DATA_FILE = 'data/data.xlsx'\n",
    "    try:\n",
    "        combined_df = pd.read_excel(COMBINED_DATA_FILE)\n",
    "        if combined_df.isnull().values.any():\n",
    "            print(\"\\n**Warning: NaN values are present in the combined DataFrame.**\")\n",
    "            rows_with_nan = combined_df[combined_df.isnull().any(axis=1)]\n",
    "            print(\"Rows with NaN values:\\n\", rows_with_nan)\n",
    "\n",
    "        # Filter out rows with 'NONE' in SUBJID\n",
    "        combined_df = combined_df[combined_df['SUBJID'] != 'NONE']\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Combined data file not found at {COMBINED_DATA_FILE}. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    # Create mappings from filename to all data\n",
    "    filename_to_all_data = {row['Filename']: row for _, row in combined_df.iterrows()}\n",
    "\n",
    "    final_airway_trees = []\n",
    "    final_targets_data = []\n",
    "    final_subj_id = []\n",
    "    aligned_graph_filenames = [] # To store filenames that successfully align\n",
    "\n",
    "    # Iterate through the graph filenames to ensure correct alignment\n",
    "    # and only include data for which both graph and target exist\n",
    "    for i, file in enumerate(graph_filenames):\n",
    "        if file in filename_to_all_data:\n",
    "            row = filename_to_all_data[file]\n",
    "            final_airway_trees.append(airway_trees_raw[i])\n",
    "            final_targets_data.append([row[target_col] for target_col in TARGET_COLS])\n",
    "            final_subj_id.append(row['SUBJID'])\n",
    "            aligned_graph_filenames.append(file) # Add filename to the aligned list\n",
    "        else:\n",
    "            logger.warning(f\"Data for file {file} not found in {COMBINED_DATA_FILE}. Skipping.\")\n",
    "\n",
    "    targets = torch.tensor(final_targets_data).float().to(device)\n",
    "    airway_trees = final_airway_trees\n",
    "\n",
    "    OUTPUT_DIR = 'models/EdgeGNN_regressor'\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    MODEL_SAVE_PATH = f'{OUTPUT_DIR}/EdgeGNN_regressor.pth'\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    num_node_features = airway_trees[0].x.shape[1]\n",
    "    output_dim = 1\n",
    "\n",
    "    target_idx = TARGET_COLS.index(TARGET_NAME)\n",
    "    processed_airway_trees = [\n",
    "        Data(x=tree.x, edge_index=tree.edge_index, edge_attr=tree.edge_attr, \n",
    "             y=torch.tensor([targets[i, target_idx]]), filename=aligned_graph_filenames[i]) # Pass filename\n",
    "        for i, tree in enumerate(airway_trees)\n",
    "    ]\n",
    "    loader_train_all = DataLoader(processed_airway_trees, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # --- Initialize and Train the GNN ---\n",
    "    model_train = AirwayEdgeGNN(\n",
    "        num_node_features=num_node_features,\n",
    "        output_channels=output_dim,\n",
    "        num_conv_layers=best_num_conv_layers,\n",
    "        mlp1_layers=best_mlp1_layers_dims,\n",
    "        mlp2_layers=best_mlp2_layers_dims,\n",
    "        mlp_activation=best_mlp_activation,\n",
    "        edgeconv_aggr=best_edgeconv_aggr,\n",
    "        use_batchnorm=best_use_batchnorm\n",
    "    ).to(device)\n",
    "    optimizer = torch.optim.Adam(model_train.parameters(), lr=best_lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if not os.path.exists(MODEL_SAVE_PATH):\n",
    "        logger.info(\"Starting GNN training...\")\n",
    "        for epoch in range(best_epochs):\n",
    "            loss = train(model_train, loader_train_all, optimizer, criterion, device)\n",
    "            logger.info(f\"Epoch {epoch+1}/{best_epochs}, Loss: {loss:.4f}\")\n",
    "        torch.save(model_train.state_dict(), MODEL_SAVE_PATH)\n",
    "        logger.info(f\"Finished GNN training. Model saved to {MODEL_SAVE_PATH}\")\n",
    "    else:\n",
    "        try:\n",
    "            model_train.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n",
    "            logger.info(f\"Loaded pre-trained GNN model from {MODEL_SAVE_PATH}\")\n",
    "        except RuntimeError as e:\n",
    "            logger.error(f\"Error loading pre-trained model: {e}\")\n",
    "            logger.info(\"Using a newly initialized model and retraining it.\")\n",
    "            for epoch in range(best_epochs):\n",
    "                loss = train(model_train, loader_train_all, optimizer, criterion, device)\n",
    "                logger.info(f\"Epoch {epoch+1}/{best_epochs}, Loss: {loss:.4f}\")\n",
    "            torch.save(model_train.state_dict(), MODEL_SAVE_PATH)\n",
    "            logger.info(f\"Finished GNN retraining. Model saved to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
