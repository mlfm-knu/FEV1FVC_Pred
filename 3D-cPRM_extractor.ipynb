{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b53380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import h5py # Not strictly needed if not reading/writing HDF5, but can keep\n",
    "import matplotlib.pyplot as plt # Not used in the provided snippet\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import nibabel as nib # For reading .nii files\n",
    "\n",
    "import CNN_model_extractor # Assuming this module defines CNN_model()\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# Configure GPU memory growth at the very top, before any TensorFlow operations\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs configured for memory growth.\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "\n",
    "def trainCNN(directory, prm_nii_dir, labels_csv_path, nb_iters=500, prm_filename_col='SUBID', target_col='End'):\n",
    "    \"\"\"\n",
    "    Trains a 3D CNN model by directly loading PRM images from .nii files\n",
    "    and labels from a .csv file.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Base directory for saving models and results.\n",
    "        prm_nii_dir (str): Directory containing the .nii PRM image files.\n",
    "        labels_csv_path (str): Path to the CSV file with 'SUBID' and 'End' columns.\n",
    "    \"\"\"\n",
    "    print(\"Starting direct loading and CNN training...\")\n",
    "\n",
    "    # 1. Read Labels from CSV and prepare data paths\n",
    "    labels_df = pd.read_csv(labels_csv_path)\n",
    "    \n",
    "    CNN_data_paths = []\n",
    "    CNN_labels = []\n",
    "    CNN_subids = [] # To store subject IDs corresponding to data paths\n",
    "\n",
    "    for index, row in labels_df.iterrows():\n",
    "        # Use 'SUBID' for the filename prefix and 'End' for the label\n",
    "        subject_id = row[prm_filename_col] \n",
    "        label = row[target_col]        \n",
    "        \n",
    "        # Construct possible .nii filenames (prioritize .nii, then .nii.gz)\n",
    "        nii_file_path = os.path.join(prm_nii_dir, f\"{subject_id}.nii\")\n",
    "        if not os.path.exists(nii_file_path):\n",
    "            nii_file_path = os.path.join(prm_nii_dir, f\"{subject_id}.nii.gz\") # Fallback to .nii.gz if .nii not found\n",
    "\n",
    "        if os.path.exists(nii_file_path):\n",
    "            CNN_data_paths.append(nii_file_path)\n",
    "            CNN_labels.append(label)\n",
    "            CNN_subids.append(subject_id) # Store the subject ID\n",
    "        else:\n",
    "            print(f\"Warning: PRM file not found for {subject_id} at {nii_file_path}. Skipping.\")\n",
    "\n",
    "    if not CNN_data_paths:\n",
    "        print(\"No valid PRM images found based on CSV. Exiting training.\")\n",
    "        return\n",
    "\n",
    "    CNN_labels = np.array(CNN_labels) # Convert labels to numpy array\n",
    "    CNN_subids_array = np.array(CNN_subids) # Convert subids to numpy array\n",
    "\n",
    "    # Define the expected 3D shape of images (from your original script)\n",
    "    expected_image_shape = (64, 64, 64) \n",
    "\n",
    "    # Setup output directories\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    os.makedirs(os.path.join(directory, \"model\", \"CNN\"), exist_ok=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=5)\n",
    "    n_iter_CNN = nb_iters\n",
    "    j = 0 # Fold counter\n",
    "    \n",
    "    # List to store evaluation results for each fold\n",
    "    all_fold_results = []\n",
    "\n",
    "    # Iterate through K-folds, splitting on file paths and labels\n",
    "    for idx_train, idx_val in kfold.split(CNN_data_paths, CNN_labels):\n",
    "        j += 1\n",
    "        accuracy = 0\n",
    "        EPOCHS = 0\n",
    "\n",
    "        train_CNN_list = []\n",
    "        val_CNN_list = []\n",
    "        val_subids_fold = CNN_subids_array[idx_val] # Get subject IDs for current validation set\n",
    "\n",
    "        # Load images for the current training set\n",
    "        for i in idx_train:\n",
    "            img_path = CNN_data_paths[i]\n",
    "            try:\n",
    "                img_data = nib.load(img_path).get_fdata().astype(np.float32)\n",
    "                if img_data.shape == expected_image_shape:\n",
    "                    train_CNN_list.append(img_data)\n",
    "                else:\n",
    "                    print(f\"Skipping {img_path} in train set: unexpected shape {img_data.shape}. Expected {expected_image_shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading train image {img_path}: {e}. Skipping.\")\n",
    "\n",
    "        # Load images for the current validation set\n",
    "        for i in idx_val:\n",
    "            img_path = CNN_data_paths[i]\n",
    "            try:\n",
    "                img_data = nib.load(img_path).get_fdata().astype(np.float32)\n",
    "                if img_data.shape == expected_image_shape:\n",
    "                    val_CNN_list.append(img_data)\n",
    "                else:\n",
    "                    print(f\"Skipping {img_path} in validation set: unexpected shape {img_data.shape}. Expected {expected_image_shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading val image {img_path}: {e}. Skipping.\")\n",
    "        \n",
    "        # Convert lists of images to NumPy arrays\n",
    "        if not train_CNN_list or not val_CNN_list:\n",
    "            print(f\"Skipping fold {j}: Not enough data loaded for training or validation.\")\n",
    "            continue\n",
    "\n",
    "        train_CNN_np = np.array(train_CNN_list)\n",
    "        val_CNN_np = np.array(val_CNN_list)\n",
    "\n",
    "        current_train_labels = CNN_labels[idx_train]\n",
    "        current_val_labels = CNN_labels[idx_val]\n",
    "\n",
    "        # Expand dimensions for labels to match (N, 1)\n",
    "        current_train_labels = np.expand_dims(current_train_labels, axis=1)\n",
    "        current_val_labels = np.expand_dims(current_val_labels, axis=1)\n",
    "\n",
    "        # Normalize data to [0, 1] range\n",
    "        min_train, max_train = np.min(train_CNN_np), np.max(train_CNN_np)\n",
    "        if (max_train - min_train) != 0:\n",
    "            train_CNN_np = (train_CNN_np - min_train) / (max_train - min_train)\n",
    "        else:\n",
    "            train_CNN_np = np.zeros_like(train_CNN_np) \n",
    "\n",
    "        min_val, max_val = np.min(val_CNN_np), np.max(val_CNN_np)\n",
    "        if (max_val - min_val) != 0:\n",
    "            val_CNN_np = (val_CNN_np - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            val_CNN_np = np.zeros_like(val_CNN_np) \n",
    "\n",
    "        print('\\n\\n###################CNN part start!!!!###################\\n\\n')\n",
    "        print('Train min=%.3f, max=%.3f' % (np.min(train_CNN_np), np.max(train_CNN_np)))\n",
    "\n",
    "        # Re-instantiate model for each fold to ensure fresh weights\n",
    "        model = CNN_model_extractor.CNN_model()\n",
    "\n",
    "        # Reshape to (batch_size, Depth, Height, Width, Channels)\n",
    "        train_CNN_reshaped = train_CNN_np.reshape(-1, *expected_image_shape, 1)\n",
    "        val_CNN_reshaped = val_CNN_np.reshape(-1, *expected_image_shape, 1)\n",
    "\n",
    "        best_accuracy_fold = 0\n",
    "        best_model_path_fold = \"\"\n",
    "\n",
    "        # Training loop for n_iter_CNN epochs\n",
    "        for i in range(0, n_iter_CNN):\n",
    "            EPOCHS += 1\n",
    "            # Adjust batch_size here for training if needed due to OOM\n",
    "            history = model.fit(train_CNN_reshaped, current_train_labels, batch_size=8, epochs=1, verbose=1) \n",
    "            # Adjust batch_size here for evaluation if needed due to OOM\n",
    "            evaluate_model = model.evaluate(val_CNN_reshaped, current_val_labels, batch_size=8, verbose=0)\n",
    "\n",
    "            if best_accuracy_fold <= evaluate_model[1]: # evaluate_model[1] is typically accuracy\n",
    "                best_accuracy_fold = evaluate_model[1]\n",
    "                print(f\"################### Fold {j} - Epoch {EPOCHS} - New Best Accuracy: {best_accuracy_fold} ###################\")\n",
    "                \n",
    "                # Save the model\n",
    "                save_path = os.path.join(directory, \"model\", \"CNN\", f'model_CNN_fold{j}_best.h5')\n",
    "                model.save(save_path)\n",
    "                best_model_path_fold = save_path\n",
    "                print(f\"Model saved to {save_path}\")\n",
    "        \n",
    "        # After training for the fold, load the best model and evaluate\n",
    "        if os.path.exists(best_model_path_fold):\n",
    "            print(f\"Loading best model for fold {j} from {best_model_path_fold} for final evaluation.\")\n",
    "            loaded_model = tf.keras.models.load_model(best_model_path_fold)\n",
    "            \n",
    "            # Predict probabilities and classes\n",
    "            y_pred_probs = loaded_model.predict(val_CNN_reshaped, batch_size=8) # Use consistent batch_size\n",
    "            y_pred_classes = (y_pred_probs > 0.5).astype(int) # Assuming binary classification with sigmoid output\n",
    "\n",
    "            # Flatten labels for sklearn metrics\n",
    "            true_labels = current_val_labels.flatten()\n",
    "            predicted_classes = y_pred_classes.flatten()\n",
    "            predicted_probs = y_pred_probs.flatten()\n",
    "\n",
    "            fold_results = {\n",
    "                'fold': j,\n",
    "                'accuracy': accuracy_score(true_labels, predicted_classes),\n",
    "                'precision': precision_score(true_labels, predicted_classes, zero_division=0),\n",
    "                'recall': recall_score(true_labels, predicted_classes, zero_division=0),\n",
    "                'f1_score': f1_score(true_labels, predicted_classes, zero_division=0),\n",
    "                'roc_auc': roc_auc_score(true_labels, predicted_probs),\n",
    "                'confusion_matrix': confusion_matrix(true_labels, predicted_classes).tolist() # Convert to list for easier saving\n",
    "            }\n",
    "            all_fold_results.append(fold_results)\n",
    "            print(f\"Fold {j} Evaluation Results: {fold_results}\")\n",
    "\n",
    "            \n",
    "        else:\n",
    "            print(f\"No best model saved for fold {j}. Skipping evaluation for this fold.\")\n",
    "\n",
    "    # Save all fold results to a text file\n",
    "    results_file_path = os.path.join(directory, \"CNN_kfold_evaluation_results.txt\")\n",
    "    with open(results_file_path, 'w') as f:\n",
    "        f.write(\"CNN K-Fold Cross-Validation Results:\\n\\n\")\n",
    "        for fold_res in all_fold_results:\n",
    "            f.write(f\"--- Fold {fold_res['fold']} ---\\n\")\n",
    "            f.write(f\"Accuracy: {fold_res['accuracy']:.4f}\\n\")\n",
    "            f.write(f\"Precision: {fold_res['precision']:.4f}\\n\")\n",
    "            f.write(f\"Recall: {fold_res['recall']:.4f}\\n\")\n",
    "            f.write(f\"F1-Score: {fold_res['f1_score']:.4f}\\n\")\n",
    "            f.write(f\"ROC AUC: {fold_res['roc_auc']:.4f}\\n\")\n",
    "            f.write(f\"Confusion Matrix: {fold_res['confusion_matrix']}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "        # Calculate and save average metrics\n",
    "        if all_fold_results:\n",
    "            avg_accuracy = np.mean([res['accuracy'] for res in all_fold_results])\n",
    "            avg_precision = np.mean([res['precision'] for res in all_fold_results])\n",
    "            avg_recall = np.mean([res['recall'] for res in all_fold_results])\n",
    "            avg_f1_score = np.mean([res['f1_score'] for res in all_fold_results])\n",
    "            avg_roc_auc = np.mean([res['roc_auc'] for res in all_fold_results])\n",
    "\n",
    "            f.write(\"\\n--- Average Metrics Across Folds ---\\n\")\n",
    "            f.write(f\"Average Accuracy: {avg_accuracy:.4f}\\n\")\n",
    "            f.write(f\"Average Precision: {avg_precision:.4f}\\n\")\n",
    "            f.write(f\"Average Recall: {avg_recall:.4f}\\n\")\n",
    "            f.write(f\"Average F1-Score: {avg_f1_score:.4f}\\n\")\n",
    "            f.write(f\"Average ROC AUC: {avg_roc_auc:.4f}\\n\")\n",
    "\n",
    "    print(f\"Evaluation results saved to {results_file_path}\")\n",
    "    print(\"CNN training and evaluation complete.\")\n",
    "\n",
    "def extract_cnn_features(model_path, prm_nii_dir, labels_csv_path, output_dir, \n",
    "                             prm_filename_col='SUBID', target_col='End', \n",
    "                             feature_layer_name='dense_1'): # Or another layer name from your CNN_model\n",
    "    \"\"\"\n",
    "    Extracts features from PRM images using a specified trained CNN model\n",
    "    and saves them along with subject IDs and labels to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the trained .h5 CNN model file.\n",
    "        prm_nii_dir (str): Directory containing the .nii PRM image files.\n",
    "        labels_csv_path (str): Path to the CSV file with 'SUBID' and 'End' columns.\n",
    "        output_dir (str): Directory where the extracted features CSV will be saved.\n",
    "        prm_filename_col (str): Column name in CSV for subject ID.\n",
    "        target_col (str): Column name in CSV for the target label.\n",
    "        feature_layer_name (str): The name of the layer from which to extract features.\n",
    "                                  Common choices: 'global_average_pooling3d' or the name\n",
    "                                  of a Dense layer before the final output.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting Feature Extraction using model: {os.path.basename(model_path)} ---\")\n",
    "\n",
    "    # Load the trained model\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: Model file not found at {model_path}.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        base_model = tf.keras.models.load_model(model_path)\n",
    "        print(f\"Successfully loaded model from {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model from {model_path}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Create a feature extraction model\n",
    "    # You need to know the name of the layer you want to extract features from.\n",
    "    # Print base_model.summary() to see layer names.\n",
    "    try:\n",
    "        feature_extractor = tf.keras.Model(\n",
    "            inputs=base_model.inputs,\n",
    "            outputs=base_model.get_layer(feature_layer_name).output\n",
    "        )\n",
    "        print(f\"Feature extractor created using layer: '{feature_layer_name}'\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error creating feature extractor: {e}\")\n",
    "        print(\"Please check if 'feature_layer_name' is a valid layer in your model.\")\n",
    "        base_model.summary() # Print summary to help user identify layer names\n",
    "        return\n",
    "\n",
    "    # Prepare data paths and labels\n",
    "    labels_df = pd.read_csv(labels_csv_path)\n",
    "    \n",
    "    CNN_data_paths = []\n",
    "    CNN_labels = []\n",
    "    CNN_subids = []\n",
    "\n",
    "    for index, row in labels_df.iterrows():\n",
    "        subject_id = row[prm_filename_col] \n",
    "        label = row[target_col]        \n",
    "        \n",
    "        nii_file_path = os.path.join(prm_nii_dir, f\"{subject_id}.nii\")\n",
    "        if not os.path.exists(nii_file_path):\n",
    "            nii_file_path = os.path.join(prm_nii_dir, f\"{subject_id}.nii.gz\")\n",
    "\n",
    "        if os.path.exists(nii_file_path):\n",
    "            CNN_data_paths.append(nii_file_path)\n",
    "            CNN_labels.append(label)\n",
    "            CNN_subids.append(subject_id)\n",
    "        else:\n",
    "            print(f\"Warning: PRM file not found for {subject_id}. Skipping for feature extraction.\")\n",
    "\n",
    "    if not CNN_data_paths:\n",
    "        print(\"No valid PRM images found based on CSV. Exiting feature extraction.\")\n",
    "        return\n",
    "\n",
    "    # Load and preprocess images for feature extraction\n",
    "    extracted_images_list = []\n",
    "    extracted_subids = []\n",
    "    extracted_labels = []\n",
    "    expected_image_shape = (64, 64, 64) \n",
    "\n",
    "    print(f\"Loading {len(CNN_data_paths)} images for feature extraction...\")\n",
    "    for i, img_path in enumerate(CNN_data_paths):\n",
    "        try:\n",
    "            img_data = nib.load(img_path).get_fdata().astype(np.float32)\n",
    "            if img_data.shape == expected_image_shape:\n",
    "                # Normalize the image data using min/max scaling\n",
    "                min_val, max_val = np.min(img_data), np.max(img_data)\n",
    "                if (max_val - min_val) != 0:\n",
    "                    img_data = (img_data - min_val) / (max_val - min_val)\n",
    "                else:\n",
    "                    img_data = np.zeros_like(img_data) # Handle cases where all pixel values are same\n",
    "\n",
    "                extracted_images_list.append(img_data)\n",
    "                extracted_subids.append(CNN_subids[i])\n",
    "                extracted_labels.append(CNN_labels[i])\n",
    "            else:\n",
    "                print(f\"Skipping {img_path}: unexpected shape {img_data.shape}. Expected {expected_image_shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}. Skipping.\")\n",
    "\n",
    "    if not extracted_images_list:\n",
    "        print(\"No images successfully loaded for feature extraction. Exiting.\")\n",
    "        return\n",
    "\n",
    "    images_np = np.array(extracted_images_list)\n",
    "    images_reshaped = images_np.reshape(-1, *expected_image_shape, 1)\n",
    "\n",
    "    # Extract features\n",
    "    print(\"Extracting features...\")\n",
    "    # Adjust batch_size here for prediction if needed due to OOM\n",
    "    features = feature_extractor.predict(images_reshaped, batch_size=8) \n",
    "    print(f\"Features shape: {features.shape}\")\n",
    "\n",
    "    if features.ndim > 2:\n",
    "        features_flat = features.reshape(features.shape[0], -1)\n",
    "    else:\n",
    "        features_flat = features # Already flat if from a Dense layer or GAP\n",
    "\n",
    "    feature_column_names = [f'cPRM_{i}' for i in range(features_flat.shape[1])]\n",
    "    \n",
    "    features_df = pd.DataFrame(features_flat, columns=feature_column_names)\n",
    "    features_df.insert(0, 'SUBID', extracted_subids)\n",
    "    features_df.insert(1, 'Label', extracted_labels) # Include the label\n",
    "\n",
    "    # Save to CSV\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_filename = os.path.join(output_dir, f'cnn_features_{os.path.basename(model_path).replace(\".h5\", \"\")}.csv')\n",
    "    features_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Extracted features saved to {output_filename}\")\n",
    "    print(\"--- Feature Extraction Complete ---\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define your actual directories and file paths\n",
    "    output_directory = 'models/cPRM_extractor' \n",
    "    prm_images_folder = 'data/PRM_607' # Directory containing CEMENT001.nii, CEMENT002.nii etc.\n",
    "    labels_csv_file = 'data/data.csv' # Path to your CSV with 'SUBID' and 'End' columns\n",
    "\n",
    "    print(\"\\n--- Running trainCNN ---\")\n",
    "    trainCNN(output_directory, prm_images_folder, labels_csv_file)\n",
    "    print(\"--- Finished trainCNN ---\")\n",
    "\n",
    "    print(\"\\n--- Running Feature Extraction ---\")\n",
    "    model_to_extract_from = os.path.join(output_directory, \"model\", \"CNN\", 'model_CNN_fold1_best.h5')\n",
    "    features_output_dir = os.path.join(output_directory, \"extracted_features\")\n",
    "\n",
    "    extract_cnn_features(\n",
    "        model_to_extract_from, \n",
    "        prm_images_folder, \n",
    "        labels_csv_file, \n",
    "        features_output_dir,\n",
    "        feature_layer_name='feature_layer_128' \n",
    "    )\n",
    "    print(\"--- Finished Feature Extraction ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
